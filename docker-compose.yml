name: "trademon"
services:
  inference:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    volumes:
      - ./inference/models:/models
    environment:
      - LLAMA_ARG_MODEL=/models/Phi-3-mini-4k-instruct-q4.gguf
      - LLAMA_ARG_N_GPU_LAYERS=999
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_API=true
    ports:
      - "8000:8000"
    entrypoint: ["/app/llama-server"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


  cache:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --save 180 1 --loglevel warning


  reader:
    build:
      context: reader
      dockerfile: .dockerfile
    volumes:
      - ./SHARED:/reader/shared
      - ./reader/app:/reader/app
      - ./reader/tests:/reader/tests
    depends_on:
      - inference
      - cache
    command: ["python", "app/main.py"]
    env_file:
      - .env
      - keys.env

  stocker:
    build:
      context: stocker
      dockerfile: .dockerfile
    volumes:
      - ./SHARED:/stocker/shared
      - ./stocker/app:/stocker/app
      - ./stocker/tests:/stocker/tests
    depends_on:
      - cache
    command: ["python", "app/main.py"]
    env_file:
      - .env
      - keys.env

  feeder:
    build:
      context: feeder
      dockerfile: .dockerfile
    volumes:
      - ./feeder/data:/feeder/data
    depends_on:
      - cache
    command: ["python", "app/main.py", "--num-points", "64"]
    env_file:
      - .env
      - keys.env

  tester:
    profiles: ["test"]
    build:
      context: ./tester
      dockerfile: .dockerfile
    volumes:
      - ./SHARED:/app/shared
    depends_on:
      - inference
      - cache
    command: "pytest -v --tb=short tests/"
    env_file:
      - .env
      - keys.env


volumes:
  redis_data:
