name: "trademon"
services:
  inference:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    volumes:
      - ./inference/models:/models
    environment:
      - LLAMA_ARG_MODEL=/models/Phi-3-mini-4k-instruct-q4.gguf
      - LLAMA_ARG_N_GPU_LAYERS=999
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_API=true
    ports:
      - "8000:8000"
    entrypoint: ["/app/llama-server"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  worker:
    build:
      context: ./worker
      dockerfile: .dockerfile
    volumes:
      - ./SHARED:/worker/shared
      - ./worker/app:/worker/app
      - ./worker/tests:/worker/tests
    depends_on:
      - inference
    entrypoint: [""]
    env_file:
      - .env
    environment:
      - MODEL_API_URL=http://inference:8000
      - PROMPT_TEMPLATES_PATH=/worker/app/prompt_templates.json
      - SEARCH_API_URL_WEB=https://api.search.brave.com/res/v1/web/search
      - SEARCH_API_URL_NEWS=https://api.search.brave.com/res/v1/news/search
      - PYTHONPATH=/worker:/worker/shared
      - REDIS_URL=redis://cache:6379/0


  cache:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --save 180 1 --loglevel warning


  worker-test:
    build:
      context: ./worker
      dockerfile: .dockerfile
    volumes:
      - ./SHARED:/worker/shared
      - ./worker/app:/worker/app
      - ./worker/tests:/worker/tests
    depends_on:
      - inference
    entrypoint: ["pytest"]
    command: ["-v", "-s", "--log-cli-level=INFO"]
    env_file:
      - .env
    environment:
      - MODEL_API_URL=http://inference:8000
      - PROMPT_TEMPLATES_PATH=/worker/app/prompt_templates.json
      - SEARCH_API_URL_WEB=https://api.search.brave.com/res/v1/web/search
      - SEARCH_API_URL_NEWS=https://api.search.brave.com/res/v1/news/search
      - PYTHONPATH=/worker:/worker/shared
      - REDIS_URL=redis://cache:6379/0


  tester:
    profiles: ["test"]
    build:
      context: ./tester
      dockerfile: .dockerfile
    volumes:
      - ./SHARED:/app/shared
    depends_on:
      - inference
    command: "pytest -v --tb=short tests/"
    environment:
      - MODEL_API_URL=http://inference:8000
      - PROMPT_TEMPLATES_PATH=/app/app/prompt_templates.json


