name: "trademon"
services:
  inference:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    volumes:
      - ./inference/models:/models
    environment:
      - LLAMA_ARG_MODEL=/models/Phi-3-mini-4k-instruct-q4.gguf
      - LLAMA_ARG_N_GPU_LAYERS=999
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_API=true
    ports:
      - "8000:8000"
    entrypoint: ["/app/llama-server"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tester:
    profiles: ["test"]
    build:
      context: ./tester
      dockerfile: .dockerfile
    depends_on:
      - inference
    command: "pytest -v --tb=short tests/"
    environment:
      - MODEL_API_URL=http://inference:8000
      - PROMPT_TEMPLATES_PATH=prompt_templates.json

